- pipeline.id: products
  # ------------ Pipeline Settings --------------
  #
  # Set the number of workers that will, in parallel, execute the filters+outputs
  # stage of the pipeline.
  #
  # This defaults to the number of the host's CPU cores.
  #pipeline number of threads, it is recommended to equal the number of cpu cores
  pipeline.workers: 4
  #
  # How many events to retrieve from inputs before sending to filters+workers
  #The number of events sent each time, the number of batch processing events, modify the default to prevent the network io overload of the es cluster
  #Default 125, the larger the value, the more efficient the data processing, but the higher the memory occupied, it can be adjusted by yourself
  pipeline.batch.size: 1000
  #
  # How long to wait before dispatching an undersized batch to filters+workers
  # Value is in milliseconds.
  #Send delay, transmission interval time, default 5
  #
  pipeline.batch.delay: 50
  # path config
  path.config: "/usr/share/logstash/pipeline/products.conf"
  #
  # ------------ Queuing Settings --------------
  #
  # Internal queuing model, "memory" for legacy in-memory based queuing and
  # "persisted" for disk-based acked queueing. Defaults is memory
  #
  queue.type: persisted
  #
  # If using queue.type: persisted, the directory path where the data files will be stored.
  # Default is path.data/queue
  #
  # path.queue:/data/es/logstash-5.6.1/data/queue
  #
  # If using queue.type: persisted, the page data files size. The queue data consists of
  # append-only data files separated into pages. Default is 250mb
  #
  queue.page_capacity: 250mb
  #
  # If using queue.type: persisted, the maximum number of unread events in the queue.
  # Default is 0 (unlimited)
  #
  queue.max_events: 0
  #
  # Specify true if you want Logstash to wait until the persistent queue is drained before shutting down
  #
  queue.drain: true
  #
  # If using queue.type: persisted, the total capacity of the queue in number of bytes.
  # If you would like more unacked events to be buffered in Logstash, you can increase the
  # capacity using this setting. Please make sure your disk drive has capacity greater than
  # the size specified here. If both max_bytes and max_events are specified, Logstash will pick
  # whichever criteria is reached first
  # Default is 1024mb or 1gb
  #
  queue.max_bytes: 1024mb
  #
  # If using queue.type: persisted, the maximum number of acked events before forcing a checkpoint
  # Default is 1024, 0 for unlimited
  #
  queue.checkpoint.acks: 1024
  #
  # If using queue.type: persisted, the maximum number of written events before forcing a checkpoint
  # Default is 1024, 0 for unlimited
  #
  queue.checkpoint.writes: 1024
  #
  # If using queue.type: persisted, the interval in milliseconds when a checkpoint is forced on the head page
  # Default is 1000, 0 for no periodic checkpoint.
  #
  queue.checkpoint.interval: 1000
  #
  # ------------ Dead-Letter Queue Settings --------------
  # Flag to turn on dead-letter queue.
  #
  dead_letter_queue.enable: true

  # If using dead_letter_queue.enable: true, the maximum size of each dead letter queue. Entries
  # will be dropped if they would increase the size of the dead letter queue beyond this setting.
  # Default is 1024mb
  dead_letter_queue.max_bytes: 1024mb

  # If using dead_letter_queue.enable: true, the directory path where the data files will be stored.
  # Default is path.data/dead_letter_queue
  #
  path.dead_letter_queue: /usr/share/logstash/data/dead_letter_queue
# https://github.com/elastic/logstash/blob/main/docs/static/performance-checklist.asciidoc
# https://blog.katastros.com/a?ID=01000-58021a99-07e9-46d3-8d9f-de46c52508c1

- pipeline.id: deadletter
  path.config: "/usr/share/logstash/pipeline/dead_letter_queue.conf"
  pipeline.workers: 1
  pipeline.batch.size: 100
